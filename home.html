<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Sleek AI Voice Assistant</title>

    <!-- Google Font - Inter -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

    <!-- Tailwind CSS CDN for utility classes -->
    <script src="https://cdn.tailwindcss.com"></script>

    <style>
        /* ===============================
           Custom CSS for Sleek Design and Animations
           =============================== */
        :root {
            --color-bg-dark: #0f172a; /* Slate-900 */
            --color-accent-blue: #3b82f6; /* Blue-500 */
            --color-accent-teal: #14b8a6; /* Teal-500 */
            --color-glass: rgba(255, 255, 255, 0.08);
            --color-text-light: #f8fafc; /* Slate-50 */
            --color-shadow-light: rgba(59, 130, 246, 0.3);
            --pulse-duration: 1.5s;
        }

        body {
            font-family: 'Inter', sans-serif;
            background: var(--color-bg-dark);
            color: var(--color-text-light);
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            margin: 0;
            overflow: hidden; /* Prevent wave overflow from causing scroll */
            padding: 20px;
        }

        /* --- Main Container --- */
        .assistant-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 40px;
            text-align: center;
            width: 100%;
            max-width: 400px; /* Constrain width on desktop */
        }

        /* --- Status Message --- */
        .status-message {
            min-height: 2.5em; /* Reserve space to prevent layout shift */
            transition: all 0.3s ease;
            font-weight: 500;
            opacity: 0.8;
            color: var(--color-text-light);
        }

        /* --- AI Bubble (Mic Button) --- */
        .ai-bubble {
            position: relative;
            width: 140px;
            height: 140px;
            border-radius: 50%;
            display: flex;
            justify-content: center;
            align-items: center;
            cursor: pointer;
            background: var(--color-accent-blue);
            transition: all 0.3s cubic-bezier(0.25, 0.46, 0.45, 0.94);
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
            border: 3px solid transparent;
            z-index: 10;
        }

        /* Hover effect */
        .ai-bubble:not(.active):hover {
            transform: scale(1.05);
            background: #60a5fa; /* Blue-400 */
        }
        
        /* Focus state */
        .ai-bubble:focus {
            outline: none;
            border-color: var(--color-accent-teal);
        }

        /* Active State (Listening/Speaking/Thinking) */
        .ai-bubble.active {
            background: var(--color-accent-teal);
            transform: scale(0.95);
            box-shadow: 0 0 0 10px var(--color-accent-teal), 0 0 30px var(--color-accent-teal);
        }

        /* Microphone Icon */
        .mic-icon {
            width: 50px;
            height: 50px;
            color: var(--color-text-light);
            transition: transform 0.3s ease;
        }

        .ai-bubble.active .mic-icon {
            /* Simple spin when active */
            transform: scale(0.8);
        }

        /* --- Pulse Animation (Ambient Glow) --- */
        .pulse-effect {
            position: absolute;
            top: 50%;
            left: 50%;
            width: 100%;
            height: 100%;
            background-color: var(--color-accent-blue);
            border-radius: 50%;
            opacity: 0;
            transform: translate(-50%, -50%);
            z-index: 9;
        }

        .ai-bubble.active .pulse-effect {
            animation: pulse-out var(--pulse-duration) infinite;
        }

        @keyframes pulse-out {
            0% { transform: translate(-50%, -50%) scale(1); opacity: 0.8; }
            100% { transform: translate(-50%, -50%) scale(1.8); opacity: 0; }
        }

        /* --- Wave Visualizer (Listening) --- */
        .waves {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 100%;
            height: 100%;
            border-radius: 50%;
            pointer-events: none;
            opacity: 0;
            transition: opacity 0.5s ease;
            z-index: 8;
        }

        .waves.visible {
            opacity: 1;
        }

        .wave {
            position: absolute;
            width: 100%;
            height: 100%;
            border: 1px solid var(--color-accent-teal);
            border-radius: 50%;
            opacity: 0;
            animation: wave-spread 2s infinite cubic-bezier(0.66, 0.0, 0.44, 1);
        }

        .wave:nth-child(2) { animation-delay: 0.5s; }
        .wave:nth-child(3) { animation-delay: 1.0s; }

        @keyframes wave-spread {
            0% { transform: scale(1); opacity: 1; border-width: 2px; }
            100% { transform: scale(3.5); opacity: 0; border-width: 0.5px; }
        }
        
        /* Stop Button Styling */
        .btn-stop {
            padding: 10px 20px;
            border-radius: 9999px; /* Pill shape */
            background: var(--color-accent-blue);
            color: var(--color-text-light);
            font-weight: 500;
            transition: all 0.2s ease;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2);
            display: flex;
            align-items: center;
            gap: 8px;
            border: none;
        }

        .btn-stop:hover {
            background: #60a5fa;
            transform: translateY(-2px);
            box-shadow: 0 6px 15px rgba(0, 0, 0, 0.3);
        }

        .btn-stop.hidden {
            visibility: hidden;
            opacity: 0;
            transform: translateY(10px);
        }

        /* --- Response Box (Glassmorphism) --- */
        .response-box {
            background: var(--color-glass);
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.15);
            border-radius: 12px;
            padding: 20px;
            width: 100%;
            max-width: 400px;
            min-height: 120px;
            text-align: left;
            opacity: 0;
            transform: translateY(20px);
            transition: all 0.5s ease-out;
            box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.37);
        }

        .response-box.visible {
            opacity: 1;
            transform: translateY(0);
        }
        
        .response-title {
            font-size: 1.125rem;
            font-weight: 600;
            color: var(--color-accent-teal);
            margin-bottom: 8px;
        }
        
        .response-content {
            font-size: 0.95rem;
            line-height: 1.5;
            color: var(--color-text-light);
            opacity: 0.9;
        }

        /* ===============================
           Mobile Responsiveness
           =============================== */
        @media (max-width: 600px) {
            .assistant-container {
                gap: 30px;
            }
            .ai-bubble {
                width: 120px;
                height: 120px;
            }
            .mic-icon {
                width: 40px;
                height: 40px;
            }
            .response-box {
                min-height: 100px;
            }
        }
    </style>
</head>
<body>

    <section class="assistant-container">
        <h1 class="text-3xl font-bold mb-4">Voice Assistant</h1>

        <!-- Status Message Area -->
        <p id="statusMessage" class="status-message">
            Tap to start recording...
        </p>

        <!-- AI Bubble / Mic Button -->
        <div class="relative flex justify-center items-center w-40 h-40">
            <button id="aiBubble" class="ai-bubble" aria-label="Start Voice Assistant" aria-pressed="false">
                <!-- Inner Pulse Effect (Visible when Active) -->
                <div class="pulse-effect"></div>

                <!-- Mic Icon (Phosphor Icons - inline SVG for portability) -->
                <svg class="mic-icon" fill="currentColor" viewBox="0 0 256 256">
                    <path d="M128,176a48,48,0,0,0,48-48V64a48,48,0,0,0-96,0v64A48,48,0,0,0,128,176ZM96,64a32,32,0,0,1,64,0v64a32,32,0,0,1-64,0Z"></path>
                    <path d="M208,128a8,8,0,0,1-16,0,64,64,0,0,0-128,0,8,8,0,0,1-16,0,80,80,0,0,1,72-80V32a8,8,0,0,1,16,0V48A80,80,0,0,1,208,128Z"></path>
                    <path d="M168,200a8,8,0,0,1-8,8H96a8,8,0,0,1-8-8,8,8,0,0,1,8-8h64A8,8,0,0,1,168,200Z"></path>
                </svg>

                <!-- Wave Visualizer (Visible when Listening) -->
                <div id="waves" class="waves">
                    <div class="wave"></div>
                    <div class="wave"></div>
                    <div class="wave"></div>
                </div>
            </button>
        </div>

        <!-- Stop Button (for manual control during long recordings/playback) -->
        <button id="stopBtn" class="btn-stop hidden" aria-hidden="true">
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><rect x="6" y="6" width="12" height="12" rx="1"/></svg>
            Stop
        </button>

        <!-- Response Display Box -->
        <div id="responseBox" class="response-box">
            <div class="response-title">AI Response</div>
            <p id="responseContent" class="response-content">The AI response will appear here after processing.</p>
        </div>

    </section>

    <!-- ========================= JavaScript â€” behavior and state ========================= -->
    <script>
        // Use an object to manage all state variables and UI elements
        const AppState = {
            STATUS_IDLE: 'Tap to start recording...',
            STATUS_LISTENING: 'Listening... (Speak now)',
            STATUS_THINKING: 'Thinking... Please wait.',
            STATUS_SPEAKING: 'AI Speaking... Tap Stop to interrupt.',
            
            ui: {
                aiBubble: document.getElementById('aiBubble'),
                waves: document.getElementById('waves'),
                stopBtn: document.getElementById('stopBtn'),
                statusMessage: document.getElementById('statusMessage'),
                responseBox: document.getElementById('responseBox'),
                responseContent: document.getElementById('responseContent'),
            },
            
            // State tracking
            isRecording: false,
            isSpeaking: false,
            mediaRecorder: null,
            audioChunks: [],
            stream: null,
            
            // Audio/Speech APIs
            speechSynth: window.speechSynthesis,
        };

        // --- State Management and UI Updates ---

        function updateUI(status) {
            AppState.ui.statusMessage.textContent = status;
            const isActive = status !== AppState.STATUS_IDLE;
            const isListening = status === AppState.STATUS_LISTENING;
            const isSpeakingOrThinking = status === AppState.STATUS_SPEAKING || status === AppState.STATUS_THINKING;
            
            // Toggle Main Bubble Active State (Pulse)
            AppState.ui.aiBubble.classList.toggle('active', isActive);
            AppState.ui.aiBubble.setAttribute('aria-pressed', String(isActive));
            
            // Toggle Waves (Listening Visualizer)
            AppState.ui.waves.classList.toggle('visible', isListening);
            
            // Toggle Stop Button
            AppState.ui.stopBtn.classList.toggle('hidden', !isActive);
            AppState.ui.stopBtn.setAttribute('aria-hidden', String(!isActive));

            // Hide response box when not speaking/thinking
            AppState.ui.responseBox.classList.toggle('visible', isSpeakingOrThinking);

            // Change bubble background based on state
            if (isSpeakingOrThinking) {
                 // Use accent teal for thinking/speaking
                AppState.ui.aiBubble.style.backgroundColor = 'var(--color-accent-teal)';
            } else {
                // Use accent blue for idle/listening
                AppState.ui.aiBubble.style.backgroundColor = 'var(--color-accent-blue)';
            }

            // Play tones for feedback
            if (status === AppState.STATUS_LISTENING) playClickTone();
            if (status === AppState.STATUS_IDLE && !AppState.isRecording && !AppState.isSpeaking) playEndTone();
        }

        // --- Core Assistant Logic ---

        async function startRecording() {
            if (AppState.isRecording || AppState.isSpeaking) return;

            try {
                // Request microphone access
                AppState.stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                AppState.isRecording = true;
                AppState.audioChunks = [];
                updateUI(AppState.STATUS_LISTENING);

                // Create MediaRecorder instance
                AppState.mediaRecorder = new MediaRecorder(AppState.stream);

                AppState.mediaRecorder.ondataavailable = (event) => {
                    AppState.audioChunks.push(event.data);
                };

                AppState.mediaRecorder.onstop = () => {
                    processAudio(); // Starts the "Thinking" state
                };

                AppState.mediaRecorder.start();
                console.log("Recording started...");

            } catch (error) {
                console.error("Error starting microphone or recording:", error);
                // Inform user about mic permission issue
                updateUI('ERROR: Please allow microphone access.');
                setTimeout(() => updateUI(AppState.STATUS_IDLE), 3000);
                AppState.isRecording = false;
            }
        }

        function stopRecording() {
            if (!AppState.isRecording) return;
            
            // Stop the MediaRecorder
            if (AppState.mediaRecorder && AppState.mediaRecorder.state !== 'inactive') {
                AppState.mediaRecorder.stop();
            }

            // Stop the microphone stream tracks
            if (AppState.stream) {
                AppState.stream.getTracks().forEach(track => track.stop());
            }

            AppState.isRecording = false;
            // The UI state change to 'Thinking' happens in processAudio()
        }
        
        function stopSpeaking() {
             if (AppState.isSpeaking) {
                AppState.speechSynth.cancel();
                AppState.isSpeaking = false;
                updateUI(AppState.STATUS_IDLE);
                console.log("Speaking manually stopped.");
            }
        }

        // The AI simulation pipeline
        async function processAudio() {
            updateUI(AppState.STATUS_THINKING);
            console.log("Processing audio...");

            // 1. Convert chunks to a single Blob
            const audioBlob = new Blob(AppState.audioChunks, { type: 'audio/webm' });

            // 2. Simulated STT (Speech-to-Text) - Replaced by real API call later
            const userQuery = await simulateSTT(audioBlob);
            
            // 3. Simulated LLM (Generative AI) - Replaced by real API call later
            const aiResponseText = await simulateAIResponse(userQuery);

            // 4. TTS (Text-to-Speech) using browser engine
            speakResponse(aiResponseText);
        }
        
        // --- API Simulation Functions (To be replaced by your backend calls) ---
        
        async function simulateSTT(audioBlob) {
            // Simulate network delay
            await new Promise(resolve => setTimeout(resolve, 800));

            const queries = [
                "What is the importance of responsive design?",
                "Tell me a short fun fact about coding.",
                "How does the Web Audio API work?",
                "Write a short, encouraging message for a developer."
            ];
            return queries[Math.floor(Math.random() * queries.length)];
        }

        async function simulateAIResponse(query) {
            // Simulate network delay
            await new Promise(resolve => setTimeout(resolve, 1500));

            let response;
            if (query.includes("responsive")) {
                response = "Responsive design ensures your application looks great on any device, from a small phone to a large monitor. It's crucial for user experience and SEO.";
            } else if (query.includes("fact")) {
                response = "Did you know the first computer bug was an actual moth found in a relay of the Harvard Mark II computer in 1947? True story!";
            } else if (query.includes("Web Audio API")) {
                response = "The Web Audio API is a powerful JavaScript system for handling audio, allowing for complex routing and mixing of audio sources, even applying effects like reverb or delay.";
            } else if (query.includes("message")) {
                response = "Keep coding! Every line of code you write, even the messy ones, is a step forward. You're building something amazing, just keep going!";
            } else {
                response = "I'm sorry, I am currently in a simulated mode and can only answer questions about web development and fun facts.";
            }
            return response;
        }

        // --- Speech Output (Browser TTS) ---

        function speakResponse(text) {
            if (!AppState.speechSynth) {
                AppState.ui.responseContent.textContent = "Error: Browser does not support Speech Synthesis.";
                updateUI(AppState.STATUS_IDLE);
                return;
            }
            
            AppState.isSpeaking = true;
            updateUI(AppState.STATUS_SPEAKING);
            
            AppState.ui.responseContent.textContent = text;
            
            const utterance = new SpeechSynthesisUtterance(text);
            
            // Optional: Find a better voice (requires the voices to be loaded)
            const voices = AppState.speechSynth.getVoices();
            const enVoice = voices.find(v => v.lang.startsWith('en'));
            if (enVoice) {
                utterance.voice = enVoice;
            }
            
            utterance.pitch = 1.0;
            utterance.rate = 1.0;

            utterance.onend = () => {
                AppState.isSpeaking = false;
                updateUI(AppState.STATUS_IDLE);
                console.log("AI finished speaking.");
            };
            
            utterance.onerror = (event) => {
                AppState.isSpeaking = false;
                updateUI(AppState.STATUS_IDLE);
                console.error('SpeechSynthesisUtterance.onerror', event);
            };

            AppState.speechSynth.speak(utterance);
        }
        
        // --- Event Listeners and Initialization ---

        function initialize() {
            // Main Button Click
            AppState.ui.aiBubble.addEventListener('click', () => {
                if (AppState.isSpeaking) {
                    stopSpeaking(); // Interrupt if speaking
                } else if (AppState.isRecording) {
                    stopRecording(); // Stop recording
                } else {
                    startRecording(); // Start recording
                }
            });

            // Stop Button Click
            AppState.ui.stopBtn.addEventListener('click', () => {
                if (AppState.isRecording) {
                    stopRecording();
                } else if (AppState.isSpeaking) {
                    stopSpeaking();
                }
            });

            updateUI(AppState.STATUS_IDLE);
        }

        // --- Web Audio Tones for Feedback ---

        function playClickTone(){
            try{
                const ctx = new (window.AudioContext || window.webkitAudioContext)();
                const o = ctx.createOscillator();
                const g = ctx.createGain();
                o.type = 'sine';
                o.frequency.setValueAtTime(440, ctx.currentTime);
                g.gain.setValueAtTime(0.0001, ctx.currentTime);
                g.gain.exponentialRampToValueAtTime(0.06, ctx.currentTime + 0.01);
                o.connect(g); g.connect(ctx.destination);
                o.start();
                g.gain.exponentialRampToValueAtTime(0.0001, ctx.currentTime + 0.25);
                o.stop(ctx.currentTime + 0.26);
            }catch(e){ console.error("WebAudio error:", e); }
        }

        function playEndTone(){
            try{
                const ctx = new (window.AudioContext || window.webkitAudioContext)();
                const o = ctx.createOscillator();
                const g = ctx.createGain();
                o.type = 'triangle';
                o.frequency.setValueAtTime(240, ctx.currentTime);
                g.gain.setValueAtTime(0.0001, ctx.currentTime);
                g.gain.exponentialRampToValueAtTime(0.05, ctx.currentTime + 0.01);
                o.connect(g); g.connect(ctx.destination);
                o.start();
                g.gain.exponentialRampToValueAtTime(0.0001, ctx.currentTime + 0.28);
                o.stop(ctx.currentTime + 0.29);
            }catch(e){ console.error("WebAudio error:", e); }
        }

        // Start the application after the window loads
        window.onload = initialize;
    </script>
</body>
</html>